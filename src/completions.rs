use super::common::LlamaCppLogitBiasType;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

pub struct CreateCompletionRequestBuilder {
    req: CreateCompletionRequest,
}
impl CreateCompletionRequestBuilder {
    pub fn new(model: impl Into<String>, prompt: impl Into<String>) -> Self {
        Self {
            req: CreateCompletionRequest {
                model: model.into(),
                prompt: prompt.into(),
                suffix: None,
                max_tokens: 16,
                temperature: None,
                top_p: None,
                n_choice: None,
                stream: false,
                logprobs: None,
                echo: false,
                stop: None,
                presence_penalty: None,
                frequency_penalty: None,
                best_of: None,
                logit_bias: None,
                user: None,
                llama_cpp_top_k: 0,
                llama_cpp_repeat_penalty: 0.0,
                llama_cpp_logit_bias_type: None,
            },
        }
    }

    pub fn build(self) -> CreateCompletionRequest {
        self.req
    }
}

/// Creates a completion for the provided prompt and parameters.
#[derive(Debug, Deserialize, Serialize)]
pub struct CreateCompletionRequest {
    model: String,
    prompt: String,

    /// The suffix that comes after a completion of inserted text. Defaults to None.
    suffix: Option<String>,
    /// The maximum number of tokens to generate in the completion. Defaults to 16.
    ///
    /// The token count of your prompt plus max_tokens cannot exceed the model's context length.
    max_tokens: u32,
    temperature: Option<f32>,
    top_p: Option<f32>,
    n_choice: Option<u32>,
    stream: bool,
    logprobs: Option<u32>,
    echo: bool,
    stop: Option<Vec<String>>,
    presence_penalty: Option<f32>,
    frequency_penalty: Option<f32>,
    best_of: Option<u32>,
    logit_bias: Option<HashMap<String, f32>>,
    user: Option<String>,

    //* llama.cpp specific parameters
    llama_cpp_top_k: i32,
    llama_cpp_repeat_penalty: f64,
    llama_cpp_logit_bias_type: Option<LlamaCppLogitBiasType>,
}

/// Represents a completion response from the API.
///
/// Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
#[derive(Debug, Deserialize, Serialize)]
pub struct CompletionObject {
    id: String,
    /// The object type, which is always "text_completion".
    object: String,
    created: u32,
    model: String,
    choices: Vec<CompletionObjectChoice>,
    usage: Vec<CompletionObjectUsage>,
}

#[derive(Debug, Deserialize, Serialize)]
pub struct CompletionObjectChoice {
    text: String,
    /// The index of the choice in the list of choices.
    index: u32,
    /// A chat completion delta generated by streamed model responses.
    logprobs: Option<u32>,
    /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `function_call` if the model called a function.
    finish_reason: String,
}

#[derive(Debug, Deserialize, Serialize)]
pub struct CompletionObjectUsage {
    /// Number of tokens in the prompt.
    prompt_tokens: u32,
    /// Number of tokens in the generated completion.
    completion_tokens: u32,
    /// Total number of tokens used in the request (prompt + completion).
    total_tokens: u32,
}
